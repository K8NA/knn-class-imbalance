{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T22:38:37.724800Z",
     "start_time": "2024-05-19T22:38:37.608168Z"
    }
   },
   "source": [
    "import openml\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import scikit-posthocs as sp\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4133996220.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn[9], line 12\u001B[1;36m\u001B[0m\n\u001B[1;33m    import scikit-posthocs as sp\u001B[0m\n\u001B[1;37m                 ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def run_benchmark(models, model_names, benchmark=\"OpenML-CC18\"):\n",
    "    results = pd.DataFrame(columns=[\"dataset\", \"model\", \"score\"]) # create dataframe for results\n",
    "    benchmark_suite = openml.study.get_suite(benchmark) # obtain the benchmark suite\n",
    "\n",
    "    subset_benchmark_suite = benchmark_suite.tasks[0:10] # subset tasks\n",
    "\n",
    "    #for task_id in benchmark_suite.tasks:  # iterate over all tasks\n",
    "    for task_id in subset_benchmark_suite: # iterate over subset tasks\n",
    "        task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "        features, targets = task.get_X_and_y()  # get the data\n",
    "        for model in range(len(models)): # iterate over all models\n",
    "            score = np.mean(cross_val_score(models[model], features, targets, cv=10, scoring=\"roc_auc_ovr\")) # calculate the metric for kfold\n",
    "            if model_names:\n",
    "                results = pd.concat([results, pd.DataFrame([[task_id, model_names[model], score]], columns=results.columns)], ignore_index=True) # insert the metric for a models and a dataset\n",
    "            else:\n",
    "                results = pd.concat([results, pd.DataFrame([[task_id, str(models[model]), score]], columns=results.columns)], ignore_index=True) # insert the metric for a models and a dataset\n",
    "    results.to_csv(\"results.csv\", index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "KNN = make_pipeline(SimpleImputer(strategy='constant'),StandardScaler(),KNeighborsClassifier())\n",
    "LR = make_pipeline(SimpleImputer(strategy='constant'),StandardScaler(),LogisticRegression())\n",
    "models = [DecisionTreeClassifier(), KNN, LR]\n",
    "model_names = [\"DT\",\"KNN\",\"LR\"]\n",
    "run_benchmark(models=models, model_names=model_names)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results = pd.read_csv(\"results.csv\")\n",
    "avg_rank = results.groupby('dataset').score.rank(pct=True).groupby(results.model).mean()\n",
    "avg_rank"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ss.friedmanchisquare(results.loc[results[\"model\"]==\"DT\",:][[\"score\"]], results.loc[results[\"model\"]==\"LR\",:][[\"score\"]], results.loc[results[\"model\"]==\"KNN\",:][[\"score\"]])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "test_results = sp.posthoc_conover_friedman(\n",
    "    results,\n",
    "    melted=True,\n",
    "    block_col='dataset',\n",
    "    group_col='model',\n",
    "    y_col='score',\n",
    ")\n",
    "sp.sign_plot(test_results)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10, 2), dpi=100)\n",
    "plt.title('Critical difference diagram of average score ranks')\n",
    "sp.critical_difference_diagram(avg_rank, test_results)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsmorph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
