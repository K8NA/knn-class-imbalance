{
 "cells": [
  {
   "cell_type": "code",
   "id": "7ecf5b07e13b6efc",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import openml\n",
    "import warnings\n",
    "\n",
    "from mla.knn import KNNClassifier as mlaKNNClassifier\n",
    "from mla.metrics import accuracy\n",
    "\n",
    "from wei import KNNClassifier as weiKNNClassifier\n",
    "from wei import KNNRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import scikit_posthocs as sp\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "warnings.simplefilter(action='ignore', category=FutureWarning)",
   "id": "86f7983126cc8df4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load dataset\n",
    "dataset = openml.datasets.get_dataset(1464, download_data=True)\n",
    "X, y, _, _ = dataset.get_data(target=dataset.default_target_attribute)"
   ],
   "id": "ba6b417002a77036",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=222)"
   ],
   "id": "6d6330df0ba2356e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize and train the model\n",
    "knn = mlaKNNClassifier(k=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)"
   ],
   "id": "447a87b74def119e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_benchmark(models, model_names, benchmark=\"OpenML-CC18\"):\n",
    "    results = pd.DataFrame(columns=[\"dataset\", \"model\", \"score\"])  # Create DataFrame for results\n",
    "    benchmark_suite = openml.study.get_suite(benchmark)  # Obtain the benchmark suite\n",
    "\n",
    "    # datasets IDs\n",
    "    # 40983\n",
    "    # 40994\n",
    "    # 1464\n",
    "    # 1487\n",
    "    # 1494\n",
    "    # 1489\n",
    "    # 1068\n",
    "    # 1067\n",
    "    # 1063\n",
    "    # 1053\n",
    "    # 1050\n",
    "    # 1049\n",
    "    subset_benchmark_suite = benchmark_suite.tasks[:10]\n",
    "\n",
    "    # Iterate over the subset of tasks\n",
    "    for task_id in subset_benchmark_suite:\n",
    "        task = openml.tasks.get_task(task_id)  # Download the OpenML task\n",
    "        features, targets = task.get_X_and_y()  # Get the data\n",
    "        for model_idx in range(len(models)):  # Iterate over all models\n",
    "            score = np.mean(cross_val_score(models[model_idx], features, targets, cv=10, scoring=\"roc_auc_ovr\"))\n",
    "            model_name = model_names[model_idx] if model_names else str(models[model_idx])\n",
    "            results = pd.concat([results, pd.DataFrame([[task_id, model_name, score]], columns=results.columns)],\n",
    "                                ignore_index=True)\n",
    "    results.to_csv(\"results.csv\", index=False)"
   ],
   "id": "baaa059145136e83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define pipelines\n",
    "KNN = make_pipeline(SimpleImputer(strategy='constant'), StandardScaler(), mlaKNNClassifier)\n",
    "KNN2 = make_pipeline(SimpleImputer(strategy='constant'), StandardScaler(), weiKNNClassifier())\n",
    "models = [KNN, KNN2]\n",
    "model_names = [\"KNN\", \"KNN2\"]"
   ],
   "id": "7be2d1f2d780989f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Run the benchmark\n",
    "run_benchmark(models=models, model_names=model_names)"
   ],
   "id": "dd8253ee94fee8cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load results and calculate average rank\n",
    "results = pd.read_csv(\"results.csv\")\n",
    "avg_rank = results.groupby('dataset').score.rank(pct=True).groupby(results.model).mean()\n",
    "print(\"avg rank: \", avg_rank)"
   ],
   "id": "eab1221684098080",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate accuracy\n",
    "acc = accuracy(y_test, y_pred)\n",
    "print(\"Accuracy: \", acc)"
   ],
   "id": "afc873da41134b5c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot cross-validation results\n",
    "def plot_cv(results_cv, metric='Accuracy'):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.boxplot(results_cv)\n",
    "    ax.set_xticklabels(results_cv.columns)\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_title('Cross-validation results for KNN and KNN2 in a dataset')\n",
    "    plt.show()"
   ],
   "id": "86290bdb47f26264",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"F1 Score: \", f1)"
   ],
   "id": "c6e5ad0378ec8879",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate and print precision, recall, and F1 score for each class\n",
    "precision = precision_score(y_test, y_pred, average=None)\n",
    "recall = recall_score(y_test, y_pred, average=None)\n",
    "f1_per_class = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"Precision per class: \", precision)\n",
    "print(\"Recall per class: \", recall)\n",
    "print(\"F1 Score per class: \", f1_per_class)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
